{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5_Precipitation_Nowcasting_PyTorch_Student.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekapolc/pattern_2022/blob/main/HW/HW05/HW5_Precipitation_Nowcasting_PyTorch_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjRBQ9voFMYx"
      },
      "source": [
        "# Precipitation Nowcasting using Neural Networks\n",
        "\n",
        "In this exercise, you are going to build a set of deep learning models on a real world task using PyTorch. PyTorch is an open source machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab (FAIR).\n",
        "\n",
        "## Setting up to use the gpu  \n",
        "\n",
        "Before we start, we need to change the environment of Colab to use GPU. Do so by:\n",
        "\n",
        "Runtime -> Change runtime type -> Hardware accelerator -> GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aelm29BaFKuc"
      },
      "source": [
        "## Deep Neural Networks with PyTorch ##\n",
        "\n",
        "To complete this exercise, you will need to build deep learning models for precipitation nowcasting. You will build a subset of the models shown below:\n",
        "- Fully Connected (Feedforward) Neural Network\n",
        "- Two-Dimentional Convolution Neural Network (2D-CNN)\n",
        "- Recurrent Neural Network with Gated Recurrent Unit (GRU)\n",
        "\n",
        "and one more model of your choice to achieve the highest score possible.\n",
        "\n",
        "We provide the code for data cleaning and some starter code for PyTorch in this notebook but feel free to modify those parts to suit your needs. Feel free to use additional libraries (e.g. scikit-learn) as long as you have a model for each type mentioned above.\n",
        "\n",
        "This notebook assumes you have already installed PyTorch with python3 and had GPU enabled. If you run this exercise on Colab you are all set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt5qwVJXFKue"
      },
      "source": [
        "## Precipitation Nowcasting ##\n",
        "\n",
        "Precipitation nowcasting is the the task of predicting the amount of rainfall in a certain region given some kind of sensor data.  The term nowcasting refers to tasks that try to predict the current or near future conditions (within 6 hours). \n",
        "\n",
        "You will be given satellite images in 3 different bands covering a 5 by 5 region from different parts of Thailand. In other words, your input will be a 5x5x3 image. Your task is to predict the amount of rainfal in the center pixel. You will first do the prediction using just a simple fully-connected neural network that view each pixel as different input features.\n",
        "\n",
        "Since the your input is basically an image, we will then view the input as an image and apply CNN to do the prediction. Finally, we can also add a time component since weather prediction can benefit greatly using previous time frames. Each data point actually contain 5 time steps, so each input data point has a size of 5x5x5x3 (time x height x width x channel), and the output data has a size of 5 (time). You will use this time information when you work with RNNs.\n",
        "\n",
        "Finally, we would like to thank the Thai Meteorological Department for providing the data for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "XdU0yrBQOQpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpV236cQtYvt"
      },
      "source": [
        "# For summarizing and visualizing models\n",
        "!pip install torchinfo\n",
        "!pip install torchviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weights and Biases\n",
        "\n",
        "[Weights and Biases](https://docs.wandb.ai/company) (wandb) is an experiment tracking tool for machine learning. It can log and visualize experiments in real time. It supports many popular ML frameworks, and obviously PyTorch is one of them. In this notebook you will learn how to log general metrics like losses, parameter distributions, and gradient distribution with wandb.\n",
        "\n",
        "To install wandb, run the cell below"
      ],
      "metadata": {
        "id": "j5F3GHHeyc8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "wcyfK7eoz0Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "1. Register [Wandb account](https://wandb.ai/login?signup=true) (and confirm your email)\n",
        "\n",
        "2. `wandb login` and copy paste the API key when prompt"
      ],
      "metadata": {
        "id": "sl8f3SBNzzDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "UMIBnplmyjdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYYloecSFKuf"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "torch.__version__ # 1.10.0+cu111"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDQFOLhM5F2k"
      },
      "source": [
        "## Loading the data\n",
        "Get the data set by going [here](https://drive.google.com/file/d/1NWR22fVVE0tO2Q5EbaPPrRKPhUem-jbw/view?usp=sharing) and click add to drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYCGn6wkF4Y5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwSDsVaNGbKa"
      },
      "source": [
        "!tar -xvf '/content/gdrive/My Drive/nowcastingHWdataset.tar.gz'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syY5DXvFFKuj"
      },
      "source": [
        "# Data Explanation #\n",
        "\n",
        "The data is an hourly measurement of water vapor in the atmosphere, and two infrared measurements of cloud imagery on a latitude-longitude coordinate. Each measurement is illustrated below as an image. These three features are included as different channels in your input data.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/burin-n/pattern-recognition/master/HW4/images/wvapor.png\" width=\"200\"> <img src=\"https://raw.githubusercontent.com/burin-n/pattern-recognition/master/HW4/images/cloud1.png\" width=\"200\"> <img src=\"https://raw.githubusercontent.com/burin-n/pattern-recognition/master/HW4/images/cloud2.png\" width=\"200\">\n",
        "\n",
        "We also provide the hourly precipitation (rainfall) records in the month of June, July, August, September, and October from weather stations spreaded around the country. A 5x5 grid around each weather station at a particular time will be paired with the precipitation recorded at the corresponding station as input and output data. Finally, five adjacent timesteps are stacked into one sequence.\n",
        "\n",
        "The month of June-August are provided as training data, while the months of September and October are used as validation and test sets, respectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6ieaQpHFKuk"
      },
      "source": [
        "# Reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg0gCg-DFKul"
      },
      "source": [
        "def read_data(months, data_dir='dataset'):\n",
        "    features = np.array([], dtype=np.float32).reshape(0,5,5,5,3)\n",
        "    labels = np.array([], dtype=np.float32).reshape(0,5)\n",
        "    for m in months:\n",
        "        filename = 'features-m{}.pk'.format(m)\n",
        "        with open(os.path.join(data_dir,filename), 'rb') as file:\n",
        "            features_temp = pickle.load(file)\n",
        "        features = np.concatenate((features, features_temp), axis=0)\n",
        "        \n",
        "        filename = 'labels-m{}.pk'.format(m)\n",
        "        with open(os.path.join(data_dir,filename), 'rb') as file:\n",
        "            labels_temp = pickle.load(file)\n",
        "        labels = np.concatenate((labels, labels_temp), axis=0)\n",
        "\n",
        "    return features, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3zFs-CXFKuo"
      },
      "source": [
        "# use data from month 6,7,8 as training set\n",
        "x_train, y_train = read_data(months=[6,7,8])\n",
        "\n",
        "# use data from month 9 as validation set\n",
        "x_val, y_val = read_data(months=[9])\n",
        "\n",
        "# use data from month 10 as test set\n",
        "x_test, y_test = read_data(months=[10])\n",
        "\n",
        "print('x_train shape:',x_train.shape)\n",
        "print('y_train shape:', y_train.shape, '\\n')\n",
        "print('x_val shape:',x_val.shape)\n",
        "print('y_val shape:', y_val.shape, '\\n')\n",
        "print('x_test shape:',x_test.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW6GD-8-FKur"
      },
      "source": [
        "**features** \n",
        "- dim 0: number of entries\n",
        "- dim 1: number of time-steps in ascending order\n",
        "- dim 2,3: a 5x5 grid around rain-measued station\n",
        "- dim 4: water vapor and two cloud imagenaries \n",
        "\n",
        "**labels**\n",
        "- dim 0: number of entries\n",
        "- dim 1: number of precipitation for each time-step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnNVxatKFKuy"
      },
      "source": [
        "# Three-Layer Feedforward Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGB0Jhk3FKuz"
      },
      "source": [
        "# Dataset need to be reshaped to make it suitable for feedforword model\n",
        "def preprocess_for_ff(x_train, y_train, x_val, y_val):\n",
        "    x_train_ff = x_train.reshape((-1, 5*5*3))\n",
        "    y_train_ff = y_train.reshape((-1, 1))\n",
        "    x_val_ff = x_val.reshape((-1, 5*5*3))\n",
        "    y_val_ff = y_val.reshape((-1, 1))\n",
        "    x_test_ff = x_test.reshape((-1, 5*5*3))\n",
        "    y_test_ff = y_test.reshape((-1, 1))\n",
        "\n",
        "    return x_train_ff, y_train_ff, x_val_ff, y_val_ff, x_test_ff, y_test_ff\n",
        "\n",
        "x_train_ff, y_train_ff, x_val_ff, y_val_ff, x_test_ff, y_test_ff = preprocess_for_ff(x_train, y_train, x_val, y_val)\n",
        "print(x_train_ff.shape, y_train_ff.shape)\n",
        "print(x_val_ff.shape, y_val_ff.shape)\n",
        "print(x_test_ff.shape, y_test_ff.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p77LrAbilkK7"
      },
      "source": [
        "### TODO#1\n",
        "\n",
        "Explain each line of code in the function preprocess_for_ff()\n",
        "\n",
        "**Ans:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "To prepare a DataLoader in order to feed data into the model, we need to create a `torch.utils.data.Dataset` object first. (Learn more about it [here](https://pytorch.org/docs/stable/data.html#map-style-datasets))\n",
        "\n",
        "Dataset is a simple class that the DataLoader will get data from, most of its functionality comes from `__getitem__(self, index)` method, which will return a single data point (both input and label). In real world scenarios the method can do some other stuffs such as\n",
        "\n",
        "1. Load images\n",
        "\n",
        "If your input (x) are images. Oftentimes you won't be able to fit all the training images into your RAM. Thus, you should pass an array (or list) of image path into the dataloader, and the `__getitem__` will be the one who dynamically loads the actual image from the harddisk for you.\n",
        "\n",
        "2. Data Normalization\n",
        "\n",
        "Data normalization helps improve stability of training. Unnormalized data can cause gradients to explode. There are many variants of normalization, but in this notebook we will use either minmax or z-score (std) normalization. Read [this](https://developers.google.com/machine-learning/data-prep/transform/normalization) (or google) if you wish to learn more about data normalization.\n",
        "\n",
        "3. Data Augmentation\n",
        "\n",
        "In computer vision, you might want to apply small changes to the images you use in training (adjust brightness, contrast, rotation) so that the model will generalize better on unseen data. There are two kinds of augmentation: static and dynamic. Static augmentation will augment images and save to disk as a new dataset. On the other hand, rather than applying the change initially and use the same change on each image every epoch, dynamic augmentation will augment each data differently for each epoch. Note that augmentation is usually done on the CPU and you might be bounded by the CPU instead. PyTorch has a dedicated [documentation about data augmentation](https://pytorch.org/vision/master/transforms.html) if you want to know more."
      ],
      "metadata": {
        "id": "1dK4x_Td1WE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RainfallDatasetFF(Dataset):\n",
        "    def __init__(self, x, y, normalizer):\n",
        "        self.x = x.astype(np.float32)\n",
        "        self.y = y.astype(np.float32)\n",
        "        self.normalizer = normalizer\n",
        "        print(self.x.shape)\n",
        "        print(self.y.shape)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.x[index] # Retrieve data\n",
        "        x = self.normalizer.transform(x.reshape(1, -1)) # Normalize\n",
        "        y = self.y[index]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]"
      ],
      "metadata": {
        "id": "TLgje5aC1Klu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCKaJISBFKus"
      },
      "source": [
        "def normalizer_std(X):\n",
        "    scaler = preprocessing.StandardScaler().fit(X)\n",
        "    return scaler\n",
        "\n",
        "def normalizer_minmax(X):\n",
        "    scaler = preprocessing.MinMaxScaler().fit(X)\n",
        "    return scaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = normalizer_std(x_train_ff) # We will normalize everything based on x_train\n",
        "\n",
        "train_dataset = RainfallDatasetFF(x_train_ff, y_train_ff, normalizer)\n",
        "val_dataset = RainfallDatasetFF(x_val_ff, y_val_ff, normalizer)\n",
        "test_dataset = RainfallDatasetFF(x_test_ff, y_test_ff, normalizer)"
      ],
      "metadata": {
        "id": "4abkl13k3TVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader\n",
        "\n",
        "DataLoader feeds data from our dataset into the model. We can freely customize batch size, data shuffle for each data split, and much more with DataLoader class. If you're curious about what can you do with PyTorch's DataLoader, you can check [this documentation](https://pytorch.org/docs/stable/data.html)"
      ],
      "metadata": {
        "id": "eewgRWXf-mJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, pin_memory=True)"
      ],
      "metadata": {
        "id": "S-eNOnvf-ne3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function\n",
        "\n",
        "PyTorch has many loss functions readily available for use. We can also write our own custom loss function as well. But for now, we will use [PyTorch's built-in mean squared error loss ](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)"
      ],
      "metadata": {
        "id": "2ETMKmwesM1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "dE0wOzDpsOeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO#2\n",
        "\n",
        "Why is the loss MSE?\n",
        "\n",
        "**Ans:**"
      ],
      "metadata": {
        "id": "R2XXPjgWoruT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device\n",
        "\n",
        "Unlike Tensorflow/Keras, PyTorch allows user to freely put any Tensor or objects (loss functions, models, optimizers, etc.) in CPU or GPU. By default, all objects created will be in CPU. In order to use GPU we will have to supply `device = torch.device(\"cuda\")` into the objects to move it to GPU. You will usually see the syntax like `object.to(device)` for moving CPU object to GPU, or `o = Object(..., device=device)` to create the object in the GPU."
      ],
      "metadata": {
        "id": "FAJdDTn_Kj2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "9nHbeRbYKk3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "Below, the code for creating a 3-layers fully connected neural network in PyTorch is provided. Run the code and make sure you understand what you are doing. Then, report the results."
      ],
      "metadata": {
        "id": "_qNgeii7-jIS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddS--sUaFKu3"
      },
      "source": [
        "class FeedForwardNN(nn.Module):\n",
        "    def __init__(self, hidden_size=200):\n",
        "        super(FeedForwardNN, self).__init__()\n",
        "        self.ff1 = nn.Linear(75, hidden_size)\n",
        "        self.ff2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.ff3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hd1 = F.relu(self.ff1(x))\n",
        "        hd2 = F.relu(self.ff2(hd1))\n",
        "        y = F.relu(self.ff3(hd2))\n",
        "        y = self.out(y)\n",
        "        return y.reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz8m8ijkmUdU"
      },
      "source": [
        "### TODO#3\n",
        "\n",
        "What is the activation function in the final dense layer? and why? Do you think there is a better activation function for the final layer?\n",
        "\n",
        "**Ans:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters and other configs\n",
        "config = {\n",
        "    'architecture': 'feedforward',\n",
        "    'lr': 0.01,\n",
        "    'hidden_size': 200,\n",
        "    'scheduler_factor': 0.2,\n",
        "    'scheduler_patience': 2,\n",
        "    'scheduler_min_lr': 1e-4,\n",
        "    'epochs': 10\n",
        "}\n",
        "\n",
        "# Model\n",
        "model_ff = FeedForwardNN(hidden_size=config['hidden_size'])\n",
        "model_ff = model_ff.to(device)\n",
        "optimizer = torch.optim.Adam(model_ff.parameters(), lr=config['lr'])\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, \n",
        "    'min', \n",
        "    factor=config['scheduler_factor'], \n",
        "    patience=config['scheduler_patience'], \n",
        "    min_lr=config['scheduler_min_lr']\n",
        ")"
      ],
      "metadata": {
        "id": "z1RKOxNBhyb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize model with torchviz\n",
        "sample_inputs = next(iter(train_loader))[0].requires_grad_(True)\n",
        "sample_y = model_ff(sample_inputs.to(device))\n",
        "make_dot(sample_y, params=dict(list(model_ff.named_parameters())+[('sample_inputs', sample_inputs)]))"
      ],
      "metadata": {
        "id": "tCa_T56Js_gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model_ff, input_size=(1024, 75))"
      ],
      "metadata": {
        "id": "4S1EzAAioGtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfnyZsPlmJW7"
      },
      "source": [
        "### TODO#4\n",
        "\n",
        "Explain why the first linear layer has number of parameters = 15200\n",
        "\n",
        "**Ans:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "WLyflo2MxrRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "learning_rates = []\n",
        "\n",
        "# Start wandb run\n",
        "wandb.init(\n",
        "    project='precipitation-nowcasting',\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "# Log parameters and gradients\n",
        "wandb.watch(model_ff, log='all')\n",
        "\n",
        "for epoch in range(config['epochs']):  # loop over the dataset multiple times\n",
        "    \n",
        "    # Training\n",
        "    train_loss = []\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    learning_rates.append(current_lr)\n",
        "\n",
        "    # Flag model as training. Some layers behave differently in training and\n",
        "    # inference modes, such as dropout, BN, etc.\n",
        "    model_ff.train()\n",
        "\n",
        "    print(f\"Training epoch {epoch+1}...\")\n",
        "    print(f\"Current LR: {current_lr}\")\n",
        "\n",
        "    for i, (inputs, y_true) in enumerate(tqdm(train_loader)):\n",
        "        # Transfer data from cpu to gpu\n",
        "        inputs = inputs.to(device)\n",
        "        y_true = y_true.to(device)\n",
        "\n",
        "        # Reset the gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Predict\n",
        "        y_pred = model_ff(inputs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_fn(y_pred, y_true)\n",
        "\n",
        "        # Compute gradient\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Log stuff\n",
        "        train_loss.append(loss)\n",
        "        \n",
        "    avg_train_loss = torch.stack(train_loss).mean().item()\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} train loss: {avg_train_loss:.4f}\")\n",
        "    \n",
        "    # Validation\n",
        "    model_ff.eval()\n",
        "    with torch.no_grad(): # No gradient is required during validation\n",
        "        print(f\"Validating epoch {epoch+1}\")\n",
        "        val_loss = []\n",
        "        for i, (inputs, y_true) in enumerate(tqdm(val_loader)):\n",
        "            # Transfer data from cpu to gpu\n",
        "            inputs = inputs.to(device)\n",
        "            y_true = y_true.to(device)\n",
        "            \n",
        "            # Predict\n",
        "            y_pred = model_ff(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_fn(y_pred, y_true)\n",
        "\n",
        "            # Log stuff\n",
        "            val_loss.append(loss)\n",
        "        \n",
        "        avg_val_loss = torch.stack(val_loss).mean().item()\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Epoch {epoch+1} val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # LR adjustment with scheduler\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Save checkpoint if val_loss is the best we got\n",
        "        best_val_loss = np.inf if epoch == 0 else min(val_losses[:-1])\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            # Save whatever you want\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model_ff.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'scheduler': scheduler.state_dict(),\n",
        "                'train_loss': avg_train_loss,\n",
        "                'val_loss': avg_val_loss,\n",
        "                'best_val_loss': best_val_loss,\n",
        "            }\n",
        "            \n",
        "            print(f\"Saving new best model..\")\n",
        "            torch.save(state, 'model_ff.pth.tar')\n",
        "    \n",
        "    wandb.log({\n",
        "        'train_loss': avg_train_loss,\n",
        "        'val_loss': avg_val_loss,\n",
        "        'lr': current_lr,\n",
        "    })\n",
        "\n",
        "wandb.finish()\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "-JqOdbMlxsKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeXGk-wHm9Nj"
      },
      "source": [
        "### TODO#5\n",
        "\n",
        "Plot loss and val_loss as a function of epochs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0iDWp6jqRRIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXmuk1a2nRSA"
      },
      "source": [
        "### TODO#6\n",
        "\n",
        "When does the model start to overfit?\n",
        "\n",
        "**Ans:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXQgJYrTne0o"
      },
      "source": [
        "### TODO#7\n",
        "\n",
        "Plot the learning rate as a function of the epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5ZdGDjJnka7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKjTiHm4o4ba"
      },
      "source": [
        "### TODO#8\n",
        "\n",
        "What makes the learning rate change?\n",
        "(hint: try to understand the scheduler [ReduceLROnPlateau](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html))\n",
        "\n",
        "\n",
        "**Ans:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model\n",
        "\n",
        "Use the code snippet below to load the model you just trained"
      ],
      "metadata": {
        "id": "Mdf-mv_v7Nl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('model_ff.pth.tar')\n",
        "loaded_model = FeedForwardNN(hidden_size=config['hidden_size']) # Create model object\n",
        "loaded_model.load_state_dict(checkpoint['model']) # Load weights\n",
        "print(f\"Loaded epoch {checkpoint['epoch']} model\")"
      ],
      "metadata": {
        "id": "CipUyLZ07PKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A more complex scheduling\n",
        "\n",
        "The scheduler can be very complicated and you can write your own heuristic for it.\n",
        "\n",
        "### TODO#9\n",
        "\n",
        "Implement a custom learning rate scheduler that behaves like the following graph.\n",
        "\n",
        "You might want to learn how to use [PyTorch's built-in learning rate schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) in order to build your own.\n",
        "\n",
        "Learning rate should be function of epoch.\n",
        "\n",
        "![](https://raw.githubusercontent.com/pjumruspun/ComProg2021-Workshop/main/graph.png)"
      ],
      "metadata": {
        "id": "JhLdhEr2ZLWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement scheduler here\n",
        "class MyScheduler():\n",
        "    def __init__(self, optimizer: torch.optim.Optimizer):\n",
        "        pass\n",
        "\n",
        "    def step(self, epoch):\n",
        "        # Changes the learning rate here\n",
        "        pass"
      ],
      "metadata": {
        "id": "YkUFFRjsaMK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now train with your scheduler\n",
        "# my_scheduler = MyScheduler(...)\n"
      ],
      "metadata": {
        "id": "3XbxPkzyaSQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JguJELuFKvL"
      },
      "source": [
        "# [Optional] Wandb #\n",
        "\n",
        "You should now have a project in wandb with the name `precipitation-nowcasting`, which you should see the latest run you just finished inside the project. If you look into the run, you should be able to see plots of learning rate, train loss, val loss in the `Charts` section. Below it should be `Gradients` and `Parameters` section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TUMxeJvFKvM"
      },
      "source": [
        "# Wandb Observation #\n",
        "\n",
        "### Optional TODO#1 \n",
        "\n",
        "Write your own interpretation of the logs from this example. A simple sentence or two for each section is sufficient.\n",
        "\n",
        "**Your answer:** "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "4opfOdo5jpXf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymW77BubFKvC"
      },
      "source": [
        "################################################################################\n",
        "# TODO#10:                                                                     #\n",
        "# Write a function to evaluate your model. Your function must predicts         #\n",
        "# using the input model and return mean square error of the model.             #\n",
        "#                                                                              #\n",
        "# Hint: Read how to use PyTorch's MSE Loss                                     #\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html              #\n",
        "################################################################################\n",
        "#                            WRITE YOUR CODE BELOW                             #\n",
        "################################################################################\n",
        "def evaluate(data_loader, model):\n",
        "    \"\"\"\n",
        "    Evaluate model on validation data given by data_loader\n",
        "    \"\"\"\n",
        "    # write code here\n",
        "\n",
        "    return mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7t7ks7vFKvF"
      },
      "source": [
        "# We will use majority rule as a baseline.\n",
        "def majority_baseline(label_set):\n",
        "    unique, counts = np.unique(label_set, return_counts=True)\n",
        "    majority = unique[np.argmax(counts)]\n",
        "    baseline = 0\n",
        "    label_set = label_set.reshape(-1,1)\n",
        "    for r in label_set:\n",
        "        baseline += (majority - r) ** 2 / len(label_set)\n",
        "    return baseline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_QU7h4lFKvJ"
      },
      "source": [
        "print('baseline')\n",
        "print('train', majority_baseline(y_train))\n",
        "print('validate', majority_baseline(y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('FF-model')\n",
        "print('train', evaluate(train_loader, model_ff).item())\n",
        "print('validate', evaluate(val_loader, model_ff).item())"
      ],
      "metadata": {
        "id": "0dUc0Y9_wHrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Ynr3VgFKvN"
      },
      "source": [
        "# Dropout #\n",
        "\n",
        "You might notice that the 3-layered feedforward does not use dropout at all. Now, try adding dropout (dropout rate of 20%) to the model, run, and report the result again.\n",
        "\n",
        "To access PyTorch's dropout, use `nn.Dropout`. Read more about PyTorch's built-in Dropout layer [here](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbnMbqT7FKvO"
      },
      "source": [
        "################################################################################\n",
        "# TODO#11:                                                                     #\n",
        "# Write a feedforward model with dropout                                       #\n",
        "################################################################################\n",
        "#                            WRITE YOUR CODE BELOW                             #\n",
        "################################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJHG3fvMFKvU"
      },
      "source": [
        "################################################################################\n",
        "# TODO#12:                                                                     #\n",
        "# Complete the code to train your dropout model                                #\n",
        "################################################################################\n",
        "print('start training ff dropout')\n",
        "################################################################################\n",
        "#                            WRITE YOUR CODE BELOW                             #\n",
        "################################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flM71KL-q9wI"
      },
      "source": [
        "### TODO#13\n",
        "\n",
        "Plot the losses and MSE of the training and validation as before. Evaluate the dropout model's performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAIRXOxTEcox"
      },
      "source": [
        "# Plot here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYJcKIDEon2c"
      },
      "source": [
        "# Evaluate\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJFdUpD8FKvc"
      },
      "source": [
        "# Convolution Neural Networks\n",
        "Now let's try to incorporate the grid sturcture to your model. Instead of passing in vectors, we are going to pass in the 5x5 grid into the model (5lat x 5long x 3channel). You are going to implement you own 2d-convolution neural networks with the following structure.\n",
        "```\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Conv2DNN                                 --                        --\n",
        "├─Conv2d: 1-1                            [1024, 200, 3, 3]         5,600\n",
        "├─Linear: 1-2                            [1024, 200]               360,200\n",
        "├─Linear: 1-3                            [1024, 200]               40,200\n",
        "├─Linear: 1-4                            [1024, 1]                 201\n",
        "==========================================================================================\n",
        "Total params: 406,201\n",
        "Trainable params: 406,201\n",
        "Non-trainable params: 0\n",
        "```\n",
        "These parameters are simple guidelines to save your time.    \n",
        "You can play with them in the final section which you can choose any normalization methods, activation function, as well as any hyperparameter the way you want.         \n",
        "\n",
        "Hint: You should read PyTorch documentation to see the list of available layers and options you can use.                         "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6vLD-bdFKvc"
      },
      "source": [
        "################################################################################\n",
        "# TODO#14:                                                                     #\n",
        "# Complete the code for preparing data for training CNN                        #\n",
        "# Input for CNN should not have time step.                                     #\n",
        "################################################################################\n",
        "#                            WRITE YOUR CODE BELOW                             #\n",
        "################################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1kmlOtfFKvg"
      },
      "source": [
        "################################################################################\n",
        "# TODO#15:                                                                     #\n",
        "# Write a PyTorch convolutional neural network model.                          #\n",
        "# You might want to use the layer torch.flatten somewhere                      #\n",
        "################################################################################\n",
        "#                            WRITE YOUR CODE BELOW                             #\n",
        "################################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-bDQL3FFKvj"
      },
      "source": [
        "################################################################################\n",
        "# TODO#16:                                                                     #\n",
        "# Complete the code to train your cnn model                                    #\n",
        "################################################################################\n",
        "print('start training conv2d')\n",
        "################################################################################\n",
        "#                            WRITE YOUR CODE BELOW                             #\n",
        "################################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FcmyjxLPrxG"
      },
      "source": [
        "# Plot losses\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n"
      ],
      "metadata": {
        "id": "jOGI2ll7xMU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEARLiH0FKvo"
      },
      "source": [
        "# Gated Recurrent Units\n",
        "\n",
        "Now, you want to add time steps into your model. Recall the original data has 5 time steps per item. You are going to pass in a data of the form 5 timesteps x 75data. This can be done using a GRU layer. Implement you own GRU network with the following structure.\n",
        "```\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "GRUModel                                 --                        --\n",
        "├─GRU: 1-1                               [1024, 5, 200]            166,200\n",
        "├─Linear: 1-2                            [1024, 5, 200]            40,200\n",
        "├─Linear: 1-3                            [1024, 5, 1]              201\n",
        "==========================================================================================\n",
        "Total params: 206,601\n",
        "Trainable params: 206,601\n",
        "Non-trainable params: 0\n",
        "```\n",
        "\n",
        "\n",
        "These parameters are simple guidelines to save your time.    \n",
        "You can play with them in the final section which you can choose any normalization methods, activation function, as well as any hyperparameter the way you want.         \n",
        "The result should be better than the feedforward model and at least on par with your CNN model.    \n",
        "\n",
        "Do consult PyTorch documentation on how to use [GRUs](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q53iJttkFKvo"
      },
      "source": [
        "################################################################################\n",
        "# TODO#17:                                                                     #\n",
        "# Complete the code for preparing data for training GRU                        #\n",
        "# GRU's input should has 3 dimensions.                                         #\n",
        "# The dimensions should compose of entries, time-step, and features.           #\n",
        "################################################################################\n",
        "#                            WRITE YOUR CODE BELOW                             #\n",
        "################################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk6hF98mFKvq"
      },
      "source": [
        "################################################################################\n",
        "# TODO#18                                                                      #\n",
        "# Write a PyTorch GRU model.                                                   #\n",
        "# Your goal is to predict a precipitation of every time step.                  #\n",
        "#                                                                              #\n",
        "# Hint: You should read PyTorch documentation to see the list of available     #\n",
        "# layers and options you can use.                                              #\n",
        "################################################################################\n",
        "#                            WRITE YOUR CODE BELOW                             #\n",
        "################################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f02waLbMFKvx"
      },
      "source": [
        "################################################################################\n",
        "# TODO#19                                                                      #\n",
        "# Complete the code to train your gru model                                    #\n",
        "################################################################################\n",
        "print('start training gru')\n",
        "################################################################################\n",
        "#                            WRITE YOUR CODE BELOW                             #\n",
        "################################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2HzoOqpP2Tc"
      },
      "source": [
        "# Plot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0Zbc4mIFKv0"
      },
      "source": [
        "# Evaluate\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkoGHlI8FKv5"
      },
      "source": [
        "# Final Section\n",
        "# PyTorch playground\n",
        "\n",
        "Now, train the best model you can do for this task. You can use any model structure and function available.    \n",
        "Remember that trainig time increases with the complexity of the model. You might find printing computation graphs helpful in debugging complicated models.    \n",
        "Your model should be better than your CNN or GRU model in the previous sections.\n",
        "\n",
        "Some ideas:\n",
        "\n",
        "- Tune the hyperparameters\n",
        "- Adding dropouts\n",
        "- Combining CNN with GRUs\n",
        "\n",
        "You should tune your model on training and validation set.    \n",
        "**The test set should be used only for the last evaluation.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prep data as you see fit\n"
      ],
      "metadata": {
        "id": "NH741odv8zOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hibvTeKFKv5"
      },
      "source": [
        "################################################################################\n",
        "# TODO#20                                                                      #\n",
        "# Write a function that returns your best PyTorch model. You can use anything  #\n",
        "# you want. The goal here is to create the best model you can think of.        #\n",
        "#                                                                              #\n",
        "# Hint: You should read PyTorch documentation to see the list of available     #\n",
        "# layers and options you can use.                                              #\n",
        "################################################################################\n",
        "#                            WRITE YOUR CODE BELOW                             #\n",
        "################################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfNpynXlFKv8"
      },
      "source": [
        "################################################################################\n",
        "# TODO#21                                                                      #\n",
        "# Complete the code to train your best model                                   #\n",
        "################################################################################\n",
        "print('start training the best model')\n",
        "################################################################################\n",
        "#                            WRITE YOUR CODE BELOW                             #\n",
        "################################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate best model on validation and test set\n"
      ],
      "metadata": {
        "id": "Ag7EC7E9MuZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOVQ2zHkFKv-"
      },
      "source": [
        "# Also evaluate your fully-connected model and CNN/GRU model on the test set.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHyzJulTFKwA"
      },
      "source": [
        "To get full credit for this part, your best model should be better than the previous models on the **test set**. \n",
        "\n",
        "### TODO#22\n",
        "\n",
        "Explain what helped and what did not help here\n",
        "\n",
        "**Ans:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Optional] Augmentation using data loader\n",
        "\n",
        "### Optional TODO#2\n",
        "\n",
        "Implement a new dataloader on your best model that will perform data augmentation. Try adding noise of zero mean and variance of $10e^{-2}$.\n",
        "\n",
        "Then, train your model."
      ],
      "metadata": {
        "id": "UqJCOYoPaZcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Dataset/DataLoader with noise here\n"
      ],
      "metadata": {
        "id": "IwEB-0wMaqhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('start training the best model with noise')\n",
        "################################################################################\n",
        "#                            WRITE YOUR CODE BELOW                             #\n",
        "################################################################################\n"
      ],
      "metadata": {
        "id": "3l44W5glBdae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model trained with noise on validation and test set\n"
      ],
      "metadata": {
        "id": "sxBScR6KBoCs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}